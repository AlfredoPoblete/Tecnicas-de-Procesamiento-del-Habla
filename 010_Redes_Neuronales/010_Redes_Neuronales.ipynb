{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preparación y Clasificación de Texto con Redes Neuronales en TensorFlow/Keras\n",
        "En el mundo del Procesamiento de Lenguaje Natural, las redes neuronales son herramientas increíblemente poderosas para comprender y generar texto. Sin embargo, antes de que podamos alimentar nuestro texto crudo a estos modelos, necesitamos transformarlo en un formato numérico que puedan entender. Este notebook te guiará a través de los pasos esenciales para preparar datos de texto y construir un modelo básico de clasificación con TensorFlow/Keras.\n",
        "\n",
        "Aprenderemos sobre:\n",
        "\n",
        "* Creación de un Dataset Sintético: Generaremos un pequeño conjunto de datos de ejemplo para ilustrar el proceso.\n",
        "* Tokenización: Convertiremos palabras en números enteros, creando un vocabulario.\n",
        "* Padding (Relleno): Aseguraremos que todas nuestras secuencias de texto tengan la misma longitud, un requisito clave para las redes neuronales.\n",
        "* Construcción y Entrenamiento de un Modelo Simple: Crearemos una red neuronal con una capa de Embedding para aprender representaciones de palabras y un clasificador para asignar etiquetas.\n",
        "* Predicción: Veremos cómo nuestro modelo puede clasificar un nuevo texto."
      ],
      "metadata": {
        "id": "yjNzg76BhcGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Importación de Librerías Necesarias\n",
        "Esta sección importa todas las bibliotecas de Python que utilizaremos en el notebook. pandas es fundamental para la manipulación de datos, mientras que tensorflow y sus módulos de preprocesamiento (Tokenizer, pad_sequences) y construcción de modelos (Sequential, Embedding, Dense, etc.) son el corazón de nuestro pipeline de PLN."
      ],
      "metadata": {
        "id": "tOoo8YE0h_pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "import pandas as pd # Importa la librería pandas, esencial para trabajar con DataFrames (estructuras de datos tabulares).\n",
        "import tensorflow as tf # Importa la librería TensorFlow, la base para construir y entrenar modelos de Machine Learning.\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Importa la clase Tokenizer para convertir texto en secuencias de números (tokens).\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Importa la función pad_sequences para asegurar que todas las secuencias de texto tengan la misma longitud."
      ],
      "metadata": {
        "id": "yj7kF9J45hzK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Creación y Guardado de un Conjunto de Datos Sintético\n",
        "Aquí, creamos un pequeño dataset de ejemplo en Python. Consiste en oraciones de texto y sus etiquetas correspondientes (0 para \"negativo\" y 1 para \"positivo\"). Luego, guardamos este dataset en un archivo CSV, simulando un escenario real donde los datos se cargan desde un archivo externo."
      ],
      "metadata": {
        "id": "zRlb8jHtiK0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bPxjobg15Hsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe54139-d947-429e-e2e8-b5f5c98df6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo 'text_dataset.csv' creado con éxito.\n"
          ]
        }
      ],
      "source": [
        "# Crear un conjunto de datos sintético y guardarlo como CSV\n",
        "# Ejemplos de texto y etiquetas\n",
        "data = {\n",
        "    \"texto\": [\n",
        "        \"No me gusta jugar a la play\",\n",
        "        \"Me encanta los dias lluviosos\",\n",
        "        \"Odio la sopa\",\n",
        "        \"Me gusta mucho las semitas\",\n",
        "        \"Prefiero café con leche que café solo\",\n",
        "        \"La comida argentina es increíble\",\n",
        "        \"Amo la electronica\",\n",
        "        \"Los días soleados son feos\",\n",
        "        \"Ver peliculas es un buen pasatiempo\",\n",
        "        \"Me gustaria aprender a cocinar\",\n",
        "        \"Es horrible el verano\",\n",
        "        \"La plaza estaba limpia, pero el día estaba caluroso\",\n",
        "        \"Se pasaron los fideos, pero salio rica la comida\",\n",
        "    ],\n",
        "    \"etiquetas\": [\n",
        "        0,  # Negativa\n",
        "        1,  # Positiva\n",
        "        0,  # Negativa\n",
        "        1,  # Positiva\n",
        "        1,  # Positiva\n",
        "        1,  # Positiva\n",
        "        1,  # Positiva\n",
        "        0,  # Positiva\n",
        "        1,  # Positiva\n",
        "        1,  # Positiva\n",
        "        0,  # Negativa\n",
        "        0,  # Negativa\n",
        "        1,  # Positiva\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convertir a DataFrame\n",
        "datos = pd.DataFrame(data)\n",
        "\n",
        "# Guardar como CSV\n",
        "datos.to_csv(\"text_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Archivo 'text_dataset.csv' creado con éxito.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Carga y Preprocesamiento del Conjunto de Datos de Texto\n",
        "Esta es la fase crucial donde preparamos el texto para que una red neuronal lo entienda. Primero, cargamos el CSV que acabamos de crear. Luego, aplicamos la tokenización para convertir palabras en números, y el padding para que todas las secuencias resultantes tengan la misma longitud, lo cual es esencial para la entrada a las capas de las redes neuronales."
      ],
      "metadata": {
        "id": "jtYdqwz_idqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos de texto\n",
        "datos = pd.read_csv(\"text_dataset.csv\")"
      ],
      "metadata": {
        "id": "WwvHF_Nm5tWe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización del texto\n",
        "# Instancia un objeto Tokenizer. num_words=5000 significa que solo se considerarán\n",
        "# las 5000 palabras más frecuentes en el vocabulario. Las palabras restantes serán ignoradas.\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "# 'fit_on_texts' construye el vocabulario interno del tokenizer basado en los textos proporcionados.\n",
        "# Asigna un ID numérico único a cada palabra.\n",
        "tokenizer.fit_on_texts(datos[\"texto\"])\n",
        "# 'texts_to_sequences' convierte cada texto en una secuencia de IDs numéricos.\n",
        "# Cada palabra se reemplaza por su ID correspondiente en el vocabulario.\n",
        "secuencias = tokenizer.texts_to_sequences(datos[\"texto\"])\n",
        "\n",
        "# Padding (Relleno) de las secuencias\n",
        "maxlen = 10  # Define la longitud máxima deseada para todas las secuencias.\n",
        "# 'pad_sequences' rellena (o trunca) las secuencias para que todas tengan 'maxlen' elementos.\n",
        "# Por defecto, el relleno se hace al principio (pre-padding) y con ceros (value=0).\n",
        "X = pad_sequences(secuencias, maxlen=maxlen)\n",
        "# Extrae las etiquetas (0 o 1) y las convierte a un array de NumPy para usarlas como salida del modelo.\n",
        "y = datos[\"etiquetas\"].values\n",
        "\n",
        "# Imprimir las primeras secuencias numéricas y el dataset preprocesado para entender el resultado\n",
        "print(\"\\nPrimeras secuencias numéricas después de tokenización:\")\n",
        "for i, seq in enumerate(secuencias[:3]):\n",
        "    print(f\"Texto original: '{datos['texto'][i]}' -> Secuencia: {seq}\")\n",
        "\n",
        "print(f\"\\nPrimeras 3 secuencias después de padding (X, con maxlen={maxlen}):\")\n",
        "print(X[:3])\n",
        "print(f\"\\nPrimeras 3 etiquetas (y): {y[:3]}\")"
      ],
      "metadata": {
        "id": "wTZ7P-b85zbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636b51ec-be9a-408a-e916-c869be83b6b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Primeras secuencias numéricas después de tokenización:\n",
            "Texto original: 'No me gusta jugar a la play' -> Secuencia: [12, 2, 5, 13, 6, 1, 14]\n",
            "Texto original: 'Me encanta los dias lluviosos' -> Secuencia: [2, 15, 3, 16, 17]\n",
            "Texto original: 'Odio la sopa' -> Secuencia: [18, 1, 19]\n",
            "\n",
            "Primeras 3 secuencias después de padding (X, con maxlen=10):\n",
            "[[ 0  0  0 12  2  5 13  6  1 14]\n",
            " [ 0  0  0  0  0  2 15  3 16 17]\n",
            " [ 0  0  0  0  0  0  0 18  1 19]]\n",
            "\n",
            "Primeras 3 etiquetas (y): [0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Construcción y Compilación del Modelo de Clasificación\n",
        "Esta sección define la arquitectura de nuestra red neuronal. Utilizamos un modelo Sequential de Keras, que es adecuado para pilas de capas simples. La capa clave aquí es la Embedding, que convierte los IDs de palabras en vectores densos y continuos, aprendiendo representaciones significativas de las palabras. Luego, se agregan capas densas para la clasificación."
      ],
      "metadata": {
        "id": "jNv1ivLViwsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiar el modelo a uno adecuado para texto\n",
        "# Se define un modelo secuencial, que es una pila lineal de capas.\n",
        "modelo = tf.keras.Sequential([\n",
        "    # Capa de Embedding:\n",
        "    # input_dim: Tamaño del vocabulario + 1 (por el 0 para padding y palabras OOV). Aquí 5000 palabras + 1.\n",
        "    # output_dim: Dimensión del vector de embedding para cada palabra (cada palabra se representará como un vector de 64 números).\n",
        "    # input_length: La longitud fija de las secuencias de entrada después del padding (maxlen).\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=maxlen),\n",
        "    # Capa GlobalAveragePooling1D:\n",
        "    # Reduce la salida 3D de la capa Embedding (batch_size, maxlen, embedding_dim) a 2D (batch_size, embedding_dim).\n",
        "    # Toma el promedio de los vectores de embedding a lo largo de la dimensión de la secuencia,\n",
        "    # resumiendo la información de la oración en un solo vector.\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    # Capa Densa (Dense):\n",
        "    # units=1: Una única neurona de salida para la clasificación binaria (positivo/negativo).\n",
        "    # activation='sigmoid': Función de activación Sigmoide. Produce una probabilidad entre 0 y 1,\n",
        "    # ideal para problemas de clasificación binaria (0 si < 0.5, 1 si >= 0.5).\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilación del modelo\n",
        "# Configura el proceso de aprendizaje del modelo.\n",
        "modelo.compile(\n",
        "    # optimizer: Algoritmo de optimización que ajusta los pesos del modelo. Adam es una buena opción por defecto.\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    # loss: Función de pérdida que el modelo intentará minimizar.\n",
        "    # 'binary_crossentropy' es ideal para problemas de clasificación binaria donde la salida es una probabilidad.\n",
        "    loss=\"binary_crossentropy\",\n",
        "    # metrics: Métricas para evaluar el rendimiento del modelo durante el entrenamiento y la evaluación.\n",
        "    # 'accuracy' (precisión) mide la proporción de predicciones correctas.\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Mostrar un resumen de la arquitectura del modelo\n",
        "modelo.summary()"
      ],
      "metadata": {
        "id": "Lx3iMaBf51X_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "d08f4a75-d054-4566-c147-7ec60bb8e958"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_2      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_2      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Entrenamiento del Modelo\n",
        "En esta etapa, el modelo aprende de los datos preprocesados. La función fit() entrena la red neuronal durante un número específico de epochs (pasadas completas sobre todo el dataset) y con un batch_size (número de muestras procesadas antes de actualizar los pesos del modelo)."
      ],
      "metadata": {
        "id": "jA5qi_rxjWVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo\n",
        "# X: Las secuencias de texto preprocesadas (entradas).\n",
        "# y: Las etiquetas correspondientes a cada secuencia (salidas esperadas).\n",
        "# epochs=15: El número de veces que el modelo iterará sobre todo el conjunto de datos de entrenamiento.\n",
        "# batch_size=2: El número de muestras que se procesarán antes de que los pesos del modelo se actualicen.\n",
        "entrenamiento = modelo.fit(X, y, epochs=15, batch_size=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXeVPkDcjbKs",
        "outputId": "8dbd0b24-19c4-4585-db4e-f05cbe8b316a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4097 - loss: 0.6945\n",
            "Epoch 2/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6825 - loss: 0.6879\n",
            "Epoch 3/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6616 - loss: 0.6786\n",
            "Epoch 4/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7491 - loss: 0.6677\n",
            "Epoch 5/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8387 - loss: 0.6480\n",
            "Epoch 6/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6929 - loss: 0.6531\n",
            "Epoch 7/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7210 - loss: 0.6398\n",
            "Epoch 8/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6616 - loss: 0.6417\n",
            "Epoch 9/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6960 - loss: 0.6332\n",
            "Epoch 10/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7918 - loss: 0.6081\n",
            "Epoch 11/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7939 - loss: 0.5909\n",
            "Epoch 12/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6564 - loss: 0.6171\n",
            "Epoch 13/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6491 - loss: 0.6108\n",
            "Epoch 14/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5783 - loss: 0.6189\n",
            "Epoch 15/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7366 - loss: 0.5689 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6) Predicción con un Nuevo Texto de Ejemplo\n",
        "Finalmente, utilizamos nuestro modelo entrenado para hacer una predicción sobre un nuevo texto. Este texto también debe pasar por los mismos pasos de tokenización y padding que los datos de entrenamiento antes de ser alimentado al modelo."
      ],
      "metadata": {
        "id": "9EJPzFMKjfcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción con ejemplo de texto\n",
        "texto_ejemplo = \"Desayunar café con leche y semitas me encanta\" # Nuevo texto para predecir.\n",
        "\n",
        "# Preprocesar el texto de ejemplo exactamente igual que los datos de entrenamiento:\n",
        "# 1. Convertir el texto a una secuencia de IDs numéricos usando el mismo tokenizer.\n",
        "secuencia_ejemplo = tokenizer.texts_to_sequences([texto_ejemplo])\n",
        "# 2. Aplicar padding para que la secuencia tenga la misma longitud máxima (maxlen).\n",
        "secuencia_ejemplo = pad_sequences(secuencia_ejemplo, maxlen=maxlen)\n",
        "\n",
        "# Realizar la predicción usando el modelo entrenado.\n",
        "# El modelo produce una probabilidad (debido a la activación sigmoide en la última capa).\n",
        "prediccion = modelo.predict(secuencia_ejemplo)\n",
        "\n",
        "# Imprimir el texto original y la probabilidad predicha.\n",
        "# prediccion[0] accede al primer (y único) resultado de la predicción, ya que solo pasamos un texto.\n",
        "print(f\"Predicción para el texto '{texto_ejemplo}': {prediccion[0]}\")\n",
        "\n",
        "# Interpretar la predicción: si la probabilidad es > 0.5, se clasifica como positiva (1); de lo contrario, como negativa (0).\n",
        "if prediccion[0] > 0.5:\n",
        "    print(f\"El modelo clasifica el texto como: Positivo (1)\")\n",
        "else:\n",
        "    print(f\"El modelo clasifica el texto como: Negativo (0)\")"
      ],
      "metadata": {
        "id": "5FuKBr-u5xs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a408816-bc72-497a-debb-e6ac3a3d4811"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Predicción para el texto 'Desayunar café con leche y semitas me encanta': [0.63094586]\n",
            "El modelo clasifica el texto como: Positivo (1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusiones\n",
        "Este notebook ha sido una demostración práctica de cómo se preprocesa el texto para que sea comprensible para las redes neuronales y cómo se construye un modelo de clasificación de texto básico.\n",
        "\n",
        "Hemos visto que:\n",
        "\n",
        "* El texto debe ser numerizado: No podemos introducir palabras directamente en una red neuronal. La tokenización es el puente entre el texto y los números.\n",
        "* La uniformidad es clave: Las redes neuronales requieren entradas de tamaño fijo. El padding resuelve este problema, aunque puede introducir ruido si las secuencias son muy cortas o truncar información si son muy largas.\n",
        "* La capa Embedding es fundamental: Esta capa es la magia detrás de cómo las redes neuronales aprenden representaciones densas y significativas de las palabras, capturando relaciones semánticas y sintácticas en el proceso de entrenamiento.\n",
        "* Modelos simples pueden aprender: Incluso con un dataset pequeño y una arquitectura sencilla (Embedding + GlobalAveragePooling1D + Dense), el modelo puede aprender a clasificar textos con una precisión razonable, demostrando el poder de las redes neuronales en PLN.\n",
        "\n",
        "Para aplicaciones del mundo real, se necesitarían datasets mucho más grandes, arquitecturas de modelos más complejas (como RNNs o Transformers), y técnicas de preprocesamiento más avanzadas. Sin embargo, los conceptos de tokenización, padding y embeddings que exploramos aquí forman la base de casi todas las aplicaciones de PLN modernas con redes neuronales."
      ],
      "metadata": {
        "id": "ca9bturXj-fc"
      }
    }
  ]
}