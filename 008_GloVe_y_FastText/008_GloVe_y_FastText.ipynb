{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cuaderno Clase PLN - GloVe y FastText\n",
        "\n",
        "**Objetivos:**\n",
        "*   Conocer enfoques alternativos a Word2Vec: GloVe (conteos globales) y FastText (subpalabras).\n",
        "*   Entender la ventaja clave de FastText para manejar palabras fuera de vocabulario (OOV - Out Of Vocabulary).\n",
        "*   Cargar y usar vectores FastText pre-entrenados.\n",
        "*   Comparar resultados y capacidades (especialmente OOV) con Word2Vec.\n",
        "*   Reflexionar sobre cómo evaluar la calidad de los embeddings y detectar sesgos.\n",
        "\n",
        "**Agenda:**\n",
        "1.  Instalaciones e Importaciones\n",
        "2.  Repaso Rápido: Word2Vec y sus limitaciones (OOV)\n",
        "3.  GloVe: Vectores Globales desde Co-ocurrencias\n",
        "4.  FastText: El Poder de las Subpalabras (¡Adiós OOV!)\n",
        "5.  Cargando Vectores FastText Pre-entrenados\n",
        "6.  Explorando FastText: Similitud, Analogías y ¡OOV!\n",
        "7.  Comparativa: Word2Vec vs FastText (foco en OOV)\n",
        "8.  Micro-Laboratorio (Ejercicio Práctico)\n",
        "9.  Brainstorming: Evaluación y Detección de Sesgos"
      ],
      "metadata": {
        "id": "Rzh9Sg9UeB_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrpz-YBJzJgn",
        "outputId": "224de070-05e1-4db3-9308-34d03a439197"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instalaciones e Importaciones"
      ],
      "metadata": {
        "id": "CpBrBQ7-ejz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necesitamos gensim principalmente\n",
        "!pip install gensim > /dev/null"
      ],
      "metadata": {
        "id": "gG4_rcP3eXrs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gensim -y # Remove the existing gensim installation\n",
        "!pip install gensim # Reinstall gensim to align with the NumPy version\n",
        "!pip install scipy==1.10.1\n",
        "# Restart the kernel to ensure the changes take effect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56qQY6vevNr",
        "outputId": "fc483b14-d131-45e1-fdd1-22b6c4ec4385"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.3.3\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scipy==1.10.1) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwqWN5fAeBAl",
        "outputId": "caaa8cf7-d3d1-4c85-bf21-a3790d0d735e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Librerías importadas.\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors, FastText # Ahora importamos FastText también\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Librerías importadas.\")\n",
        "\n",
        "# **Importante:** Para este cuaderno, idealmente necesitaremos:\n",
        "# 1. El modelo Word2Vec cargado previamente (para comparar).\n",
        "# 2. Un modelo FastText pre-entrenado para español (¡necesitamos descargarlo!)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Repaso Rápido: Word2Vec y sus limitaciones (OOV)\n",
        "\n",
        "Recordemos:\n",
        "*   Word2Vec (CBOW/Skip-gram) aprende embeddings prediciendo palabras en contextos locales.\n",
        "*   Genera vectores densos que capturan semántica (similitud, analogías).\n",
        "*   Usamos modelos pre-entrenados porque entrenarlos es costoso.\n",
        "\n",
        "**Una limitación importante:** Word2Vec asigna un vector a cada palabra *completa* que vio durante el entrenamiento. Si en tiempo de uso aparece una palabra que **NO estaba en el vocabulario original** (Out-Of-Vocabulary - OOV):\n",
        "*   Un error tipográfico (\"computadorra\").\n",
        "*   Una palabra nueva o muy rara (\"covid\", \"guasap\").\n",
        "*   Una variante morfológica no vista (\"cantábamos\").\n",
        "\n",
        "**¿Qué pasa con Word2Vec?** ¡Generalmente da un error (`KeyError`) o asigna un vector genérico de \"desconocido\" (UNK - Unknown), perdiendo toda información específica! Esto es un problema en aplicaciones reales."
      ],
      "metadata": {
        "id": "U0plow6ye_fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. GloVe: Vectores Globales desde Co-ocurrencias\n",
        "\n",
        "GloVe (Global Vectors for Word Representation) de Stanford es otra técnica popular para obtener embeddings.\n",
        "\n",
        "*   **Enfoque Diferente:** En lugar de predecir contextos locales (como Word2Vec), GloVe se enfoca en las **estadísticas globales de co-ocurrencia** de palabras en todo el corpus.\n",
        "*   **Idea Central:** La *relación* entre las probabilidades de co-ocurrencia de dos palabras con una tercera palabra \"sonda\" puede revelar información sobre su significado. Por ejemplo, la probabilidad de que \"hielo\" aparezca cerca de \"sólido\" será alta, mientras que cerca de \"gas\" será baja. La probabilidad de que \"vapor\" aparezca cerca de \"gas\" será alta y cerca de \"sólido\" baja. GloVe modela estas *proporciones* de probabilidades.\n",
        "*   **Entrenamiento:** Construye una matriz gigante de co-ocurrencia (cuántas veces la palabra X aparece cerca de la palabra Y) y luego usa factorización de matrices para encontrar los vectores de palabras que mejor explican esa matriz.\n",
        "*   **Resultado:** Embeddings densos similares a Word2Vec, a menudo muy buenos para tareas de similitud y analogía.\n",
        "*   **Limitación OOV:** Al igual que Word2Vec, GloVe tradicionalmente opera a nivel de palabra completa, por lo que también sufre del problema de OOV.\n",
        "\n",
        "*(Nota: Encontrar modelos GloVe pre-entrenados y de buena calidad para español puede ser más difícil que para Word2Vec o FastText. A menudo se distribuyen en formato texto).*"
      ],
      "metadata": {
        "id": "l8cCxcvmfc39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. FastText: El Poder de las Subpalabras (¡Adiós OOV!)\n",
        "\n",
        "FastText, desarrollado por Facebook AI Research (FAIR), es una extensión inteligente de Word2Vec (Skip-gram) que aborda directamente el problema OOV.\n",
        "\n",
        "*   **¡La Gran Idea!:** FastText no trata las palabras como unidades indivisibles. Representa cada palabra como una **bolsa de n-gramas de caracteres**, además de la palabra completa.\n",
        "    *   Ejemplo: Para la palabra \"amarillo\" y usando n=3 (trigramas):\n",
        "        *   Se consideran n-gramas como: `<am`, `ama`, `mar`, `ari`, `ril`, `ill`, `llo`, `lo>` (con `<` y `>` marcando inicio y fin).\n",
        "        *   También se considera la palabra completa: `<amarillo>`\n",
        "*   **Aprendizaje:** El modelo aprende vectores no solo para las palabras completas, sino **para todos los n-gramas de caracteres** vistos en el corpus.\n",
        "*   **Vector Final:** El vector de una palabra se obtiene **sumando los vectores de todos sus n-gramas de caracteres** (y el vector de la palabra completa si existe).\n",
        "\n",
        "**¡La Ventaja Clave - Manejo de OOV!**\n",
        "*   Si aparece una palabra nueva que no estaba en el vocabulario (ej: \"amarillento\"), FastText **aún puede construir un vector para ella**. ¿Cómo? Sumando los vectores de los n-gramas que sí conoce y que forman parte de la palabra nueva (ej: `<am`, `ama`, `mar`, `ari`, `ril`, `ill`, `lle`, `len`, `ent`, `nto`, `to>`).\n",
        "*   El vector resultante captura información de las \"partes\" de la palabra, lo que a menudo resulta en una representación razonable incluso para palabras desconocidas.\n",
        "\n",
        "**Otros Beneficios:**\n",
        "*   Funciona muy bien para **lenguajes morfológicamente ricos** (como el español) donde muchas palabras comparten raíces y afijos (ej: \"nación\", \"nacional\", \"nacionalidad\"). Comparten n-gramas, sus vectores estarán relacionados.\n",
        "*   Es más robusto a **errores tipográficos** (\"computadorra\" compartirá muchos n-gramas con \"computadora\")."
      ],
      "metadata": {
        "id": "94o6eSPaf5Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Cargando Vectores FastText Pre-entrenados\n",
        "\n",
        "Al igual que con Word2Vec, lo usual es usar modelos FastText pre-entrenados.\n",
        "\n",
        "**Tarea:** Descargar los vectores pre-entrenados para **español** desde el sitio oficial de FastText: [https://fasttext.cc/docs/en/pretrained-vectors.html](https://fasttext.cc/docs/en/pretrained-vectors.html)\n",
        "\n",
        "*   **Importante:** Descargar el archivo `.bin`. Este formato contiene no solo los vectores de las palabras del vocabulario, sino también la información de los n-gramas necesaria para calcular vectores de palabras OOV. El formato `.vec` es más pequeño pero pierde esta capacidad OOV.\n",
        "*   El archivo `.bin` para español también es grande (varios GB). Anoten la ruta donde lo guardan.\n",
        "\n",
        "**¡Reemplacen `'RUTA_AL_ARCHIVO_FASTTEXT.bin'` con la ruta real!**"
      ],
      "metadata": {
        "id": "nU1XFPYAgZDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ¡¡¡ MODIFICAR ESTA RUTA !!! ---\n",
        "# Ejemplo de ruta (si lo subieron a Colab o está en el mismo directorio):\n",
        "# path_to_fasttext = 'cc.es.300.bin' # Nombre común del archivo FastText para español\n",
        "# Ejemplo de ruta (si está en Google Drive montado):\n",
        "path_to_fasttext = '/content/drive/MyDrive/wiki.es.bin' # Ajustar según su estructura"
      ],
      "metadata": {
        "id": "_x3-luubmFb2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intentar cargar el modelo FastText\n",
        "try:\n",
        "    print(\"Cargando vectores FastText (.bin)... (¡Esto puede tardar MUCHO tiempo y consumir RAM!)\")\n",
        "    # Usamos FastText.load_fasttext_format para cargar modelos .bin de FastText\n",
        "    fasttext_model = gensim.models.fasttext.load_facebook_model(path_to_fasttext)\n",
        "    # Los vectores están dentro del atributo .wv (como en Word2Vec cargado con KeyedVectors)\n",
        "    fasttext_vectors = fasttext_model.wv\n",
        "    print(f\"¡Vectores FastText cargados! Vocabulario (estimado): {len(fasttext_vectors.index_to_key)} palabras. Dimensión: {fasttext_vectors.vector_size}\")\n",
        "    # Nota: El tamaño del vocabulario explícito puede ser menor en .bin, pero puede generar para OOV.\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: No se encontró el archivo FastText en la ruta '{path_to_fasttext}'.\")\n",
        "    print(\"Por favor, descarga el archivo .bin pre-entrenado para español desde el sitio de FastText\")\n",
        "    print(\"y asegúrate de que la variable 'path_to_fasttext' tenga la ruta correcta.\")\n",
        "    fasttext_vectors = None\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error al cargar el modelo FastText: {e}\")\n",
        "    fasttext_vectors = None"
      ],
      "metadata": {
        "id": "LnkWSkrpmBT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065ab9c6-5817-405d-cb80-acc2ad7c2eaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando vectores FastText (.bin)... (¡Esto puede tardar MUCHO tiempo y consumir RAM!)\n",
            "¡Vectores FastText cargados! Vocabulario (estimado): 985667 palabras. Dimensión: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# También intentemos recargar el modelo Word2Vec del martes para comparar\n",
        "# --- ¡¡¡ MODIFICAR ESTA RUTA TAMBIÉN SI ES NECESARIO !!! ---\n",
        "path_to_word2vec = '/content/drive/MyDrive/SBW-vectors-300-min5.bin.gz' # La ruta del martes\n",
        "word2vec_vectors = None"
      ],
      "metadata": {
        "id": "KjLgGPmOmN8N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(\"\\nRecargando vectores Word2Vec del martes...\")\n",
        "    word2vec_vectors = KeyedVectors.load_word2vec_format(path_to_word2vec, binary=True)\n",
        "    print(\"Vectores Word2Vec recargados.\")\n",
        "except Exception as e:\n",
        "    print(f\"No se pudo recargar Word2Vec desde '{path_to_word2vec}': {e}\")"
      ],
      "metadata": {
        "id": "L9YzR49AmKAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbdcbb2-0f75-47a7-f37e-0a5c64899842"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recargando vectores Word2Vec del martes...\n",
            "Vectores Word2Vec recargados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Explorando FastText: Similitud, Analogías y ¡OOV!\n",
        "\n",
        "Si `fasttext_vectors` se cargó, podemos hacer pruebas similares a las de Word2Vec, pero prestando especial atención a las palabras OOV."
      ],
      "metadata": {
        "id": "7CZhEmSemSPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fasttext_vectors:\n",
        "    # --- Pruebas estándar (similitud, analogía) ---\n",
        "    print(\"\\n--- Explorando FastText ---\")\n",
        "    # Similitud\n",
        "    try:\n",
        "        similares_rey_ft = fasttext_vectors.most_similar('rey', topn=5)\n",
        "        print(\"FastText - Más similares a 'rey':\", similares_rey_ft)\n",
        "    except KeyError:\n",
        "        print(\"FastText - Palabra 'rey' no encontrada.\") # Raro si el modelo es bueno\n",
        "\n",
        "    # Analogía\n",
        "    try:\n",
        "        analogia_reina_ft = fasttext_vectors.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=1)\n",
        "        print(\"FastText - rey - hombre + mujer ≈\", analogia_reina_ft)\n",
        "    except KeyError as e:\n",
        "        print(f\"FastText - Error en analogía rey/reina: falta la palabra '{e.args[0]}'\")\n",
        "\n",
        "\n",
        "    # --- ¡La prueba OOV! ---\n",
        "    print(\"\\n--- Probando Palabras Fuera de Vocabulario (OOV) ---\")\n",
        "    palabras_oov = [\n",
        "        \"computadorra\", # Error tipográfico\n",
        "        \"wasapear\",     # Neologismo / Palabra coloquial\n",
        "        \"covid\",        # Palabra relativamente nueva (depende del corpus)\n",
        "        \"programadorazo\", # Palabra inventada con sufijo común\n",
        "        \"desayunábamos\", # Forma verbal menos frecuente\n",
        "        \"internauta\"    # Palabra estándar, veamos si está\n",
        "    ]\n",
        "\n",
        "    for palabra in palabras_oov:\n",
        "        print(f\"\\nPalabra OOV: '{palabra}'\")\n",
        "        # ¿Está explícitamente en el vocabulario? (Puede que sí si el corpus era reciente)\n",
        "        in_vocab = palabra in fasttext_vectors\n",
        "        print(f\"  ¿En vocabulario explícito? {in_vocab}\")\n",
        "\n",
        "        # Intentar obtener el vector (¡FastText debería poder!)\n",
        "        try:\n",
        "            vector_oov = fasttext_vectors[palabra]\n",
        "            print(f\"  ¡Vector obtenido! (Dim: {vector_oov.shape})\")\n",
        "\n",
        "            # Encontrar similares basados en el vector generado\n",
        "            similares_oov = fasttext_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"  Similares (FastText): {similares_oov}\")\n",
        "        except Exception as e:\n",
        "            # Este error no debería ocurrir con un modelo .bin cargado correctamente\n",
        "            print(f\"  Error inesperado obteniendo vector/similares para '{palabra}': {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar los vectores FastText. Saltando la exploración.\")"
      ],
      "metadata": {
        "id": "gUe0jsFLmTTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e847b4f-7faf-4b13-a793-e9b0134c1b13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Explorando FastText ---\n",
            "FastText - Más similares a 'rey': [('monarca', 0.7459232807159424), ('rey―', 0.6321003437042236), ('exmonarca', 0.6310685873031616), ('rey,', 0.6303314566612244), ('reina', 0.6260297298431396)]\n",
            "FastText - rey - hombre + mujer ≈ [('reina', 0.6612293720245361)]\n",
            "\n",
            "--- Probando Palabras Fuera de Vocabulario (OOV) ---\n",
            "\n",
            "Palabra OOV: 'computadorra'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('computador', 0.8841244578361511), ('computadora/ordenador', 0.8638773560523987), ('computadora,', 0.8576205372810364)]\n",
            "\n",
            "Palabra OOV: 'wasapear'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('dissapear', 0.6578547358512878), ('tapear', 0.6350954174995422), ('papear', 0.5647345781326294)]\n",
            "\n",
            "Palabra OOV: 'covid'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('covides', 0.7112458944320679), ('coview', 0.6201539635658264), ('covida', 0.5898890495300293)]\n",
            "\n",
            "Palabra OOV: 'programadorazo'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('programadora', 0.8951981663703918), ('programadoras', 0.8484038710594177), ('programadorcccp', 0.8139643669128418)]\n",
            "\n",
            "Palabra OOV: 'desayunábamos'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('dábamos', 0.7762657999992371), ('tomábamos', 0.7618230581283569), ('grabábamos', 0.7587261199951172)]\n",
            "\n",
            "Palabra OOV: 'internauta'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('internautas', 0.8382586240768433), ('internaute', 0.8171950578689575), ('cibernauta', 0.7792933583259583)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Comparativa: Word2Vec vs FastText (foco en OOV)\n",
        "\n",
        "Ahora, si ambos modelos (`word2vec_vectors` y `fasttext_vectors`) están cargados, podemos comparar directamente su comportamiento con palabras OOV."
      ],
      "metadata": {
        "id": "IvE8aywDmXio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Comparación directa OOV ---\n",
        "if word2vec_vectors and fasttext_vectors:\n",
        "    print(\"\\n--- Comparación OOV: Word2Vec vs FastText ---\")\n",
        "\n",
        "    # Reutilizamos la lista de palabras OOV\n",
        "    palabras_oov = [\n",
        "        \"computadorra\", \"wasapear\", \"covid\", \"programadorazo\", \"desayunábamos\", \"internauta\"\n",
        "    ]\n",
        "\n",
        "    for palabra in palabras_oov:\n",
        "        print(f\"\\n--- Palabra: '{palabra}' ---\")\n",
        "\n",
        "        # Intentar con Word2Vec\n",
        "        print(\"  Intentando con Word2Vec:\")\n",
        "        try:\n",
        "            vector_w2v = word2vec_vectors[palabra]\n",
        "            similares_w2v = word2vec_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"    ¡Encontrada! Vector: {vector_w2v.shape}, Similares: {similares_w2v}\")\n",
        "        except KeyError:\n",
        "            print(\"    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR inesperado con Word2Vec: {e}\")\n",
        "\n",
        "        # Intentar con FastText\n",
        "        print(\"  Intentando con FastText:\")\n",
        "        try:\n",
        "            vector_ft = fasttext_vectors[palabra]\n",
        "            similares_ft = fasttext_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"    ¡Vector generado! Vector: {vector_ft.shape}, Similares: {similares_ft}\")\n",
        "        except Exception as e:\n",
        "            # No debería fallar por KeyError, pero podría haber otro error\n",
        "            print(f\"    ERROR inesperado con FastText: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar ambos modelos (Word2Vec y FastText) para comparar.\")"
      ],
      "metadata": {
        "id": "Znmbfy7BmZgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5565821c-9c84-4b98-9f4f-5bf87046d33a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación OOV: Word2Vec vs FastText ---\n",
            "\n",
            "--- Palabra: 'computadorra' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('computador', 0.8841244578361511), ('computadora/ordenador', 0.8638773560523987), ('computadora,', 0.8576205372810364)]\n",
            "\n",
            "--- Palabra: 'wasapear' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('dissapear', 0.6578547358512878), ('tapear', 0.6350954174995422), ('papear', 0.5647345781326294)]\n",
            "\n",
            "--- Palabra: 'covid' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('covides', 0.7112458944320679), ('coview', 0.6201539635658264), ('covida', 0.5898890495300293)]\n",
            "\n",
            "--- Palabra: 'programadorazo' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('programadora', 0.8951981663703918), ('programadoras', 0.8484038710594177), ('programadorcccp', 0.8139643669128418)]\n",
            "\n",
            "--- Palabra: 'desayunábamos' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ¡Encontrada! Vector: (300,), Similares: [('charlábamos', 0.7706557512283325), ('cenábamos', 0.7697142958641052), ('despertábamos', 0.769340455532074)]\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('dábamos', 0.7762657999992371), ('tomábamos', 0.7618230581283569), ('grabábamos', 0.7587261199951172)]\n",
            "\n",
            "--- Palabra: 'internauta' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ¡Encontrada! Vector: (300,), Similares: [('cibernauta', 0.6111046671867371), ('Tianya', 0.5805689692497253), ('blogger', 0.5799006819725037)]\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('internautas', 0.8382586240768433), ('internaute', 0.8171950578689575), ('cibernauta', 0.7792933583259583)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión de la Comparativa:**\n",
        "\n",
        "Deberían observar que para la mayoría (o todas) las palabras OOV, Word2Vec lanza un `KeyError`, mientras que FastText logra generar un vector y encontrar palabras similares (que pueden o no ser muy coherentes, dependiendo de qué tan \"rara\" sea la palabra OOV, pero *intenta* algo basado en sus partes).\n",
        "\n",
        "**¿Cuándo usar FastText?**\n",
        "*   Cuando trabajas con texto ruidoso (redes sociales, OCR) con typos.\n",
        "*   Cuando tratas con lenguajes morfológicamente ricos (español, alemán, etc.).\n",
        "*   Cuando esperas encontrar palabras nuevas o raras en tus datos de aplicación.\n",
        "*   **En general, para español, FastText suele ser una opción más robusta y recomendable que Word2Vec o GloVe.**\n",
        "\n",
        "**Desventaja:** Los modelos `.bin` de FastText son más grandes en disco y consumen más RAM porque almacenan información de los n-gramas."
      ],
      "metadata": {
        "id": "uU57C8I8mcdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:** (Asumiendo que `fasttext_vectors` y `word2vec_vectors` están cargados)\n",
        "\n",
        "1.  **Comparación de Resultados:**\n",
        "    *   Elegir 3 palabras que **sí** estén en ambos vocabularios (ej: 'gato', 'correr', 'inteligencia').\n",
        "    *   Para cada palabra, obtener las 5 más similares usando `word2vec_vectors.most_similar()` y `fasttext_vectors.most_similar()`.\n",
        "    *   Comparar las listas de similares. ¿Son idénticas? ¿Muy parecidas? ¿Diferentes? ¿Cuál les parece \"mejor\" o más coherente? Anotar observaciones.\n",
        "\n",
        "2.  **Test OOV Exhaustivo:**\n",
        "    *   Crear una lista propia de 10 palabras OOV. Incluyan:\n",
        "        *   Errores tipográficos comunes (ej: \"hobmre\", \"qeu\", \"dicimbre\").\n",
        "        *   Diminutivos/Aumentativos (ej: \"perrito\", \"casita\", \"libraco\").\n",
        "        *   Formas verbales conjugadas (ej: \"habíamos comido\", \"cantasteis\").\n",
        "        *   Palabras inventadas pero plausibles (ej: \"tecnoestrés\", \"computofilia\").\n",
        "    *   Para **cada** palabra OOV de su lista:\n",
        "        *   Verificar si da `KeyError` en `word2vec_vectors`.\n",
        "        *   Obtener las 3 palabras más similares usando `fasttext_vectors`. Anotar los resultados. ¿Los similares que da FastText tienen algún sentido basado en las partes de la palabra OOV?\n",
        "\n",
        "3.  **Discusión:**\n",
        "    *   ¿En qué tipo de aplicación real (ej: un chatbot de atención al cliente, un sistema de recomendación de noticias, un corrector ortográfico) creen que la capacidad OOV de FastText marcaría una diferencia significativa respecto a usar Word2Vec? ¿Por qué?"
      ],
      "metadata": {
        "id": "6bPYX6xPmdU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Comparación de Resultados para Palabras en Vocabulario\n",
        "Vamos a elegir tres palabras comunes que esperamos que estén en ambos vocabularios y comparar sus similares. Usaré ejemplos que probablemente estén en modelos grandes en inglés, ya que son más accesibles."
      ],
      "metadata": {
        "id": "baqfOUV9_1TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fasttext_vectors and word2vec_vectors:\n",
        "    print(\"\\n--- Comparación de Similares para Palabras en Vocabulario ---\")\n",
        "    palabras_en_vocab = ['perro', 'volar', 'artificial'] # Elegidas para estar en modelos comunes en inglés\n",
        "\n",
        "    for palabra in palabras_en_vocab:\n",
        "        print(f\"\\nPalabra: '{palabra}'\")\n",
        "\n",
        "        # Similares con Word2Vec\n",
        "        print(\"  Similares (Word2Vec):\")\n",
        "        try:\n",
        "            similares_w2v = word2vec_vectors.most_similar(palabra, topn=5)\n",
        "            for sim_word, score in similares_w2v:\n",
        "                print(f\"    - {sim_word}: {score:.4f}\")\n",
        "        except KeyError:\n",
        "            print(\"    Palabra no encontrada en Word2Vec (esto no debería pasar para palabras comunes).\")\n",
        "\n",
        "        # Similares con FastText\n",
        "        print(\"  Similares (FastText):\")\n",
        "        try:\n",
        "            similares_ft = fasttext_vectors.most_similar(palabra, topn=5)\n",
        "            for sim_word, score in similares_ft:\n",
        "                print(f\"    - {sim_word}: {score:.4f}\")\n",
        "        except KeyError:\n",
        "            print(\"    Palabra no encontrada en FastText (esto no debería pasar para palabras comunes).\")\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar ambos modelos para la comparación en vocabulario.\")"
      ],
      "metadata": {
        "id": "26vXMK1amgTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4df4415-caa9-41d5-81a9-0d28bd1ef45f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación de Similares para Palabras en Vocabulario ---\n",
            "\n",
            "Palabra: 'perro'\n",
            "  Similares (Word2Vec):\n",
            "    - gato: 0.8182\n",
            "    - cachorro: 0.7781\n",
            "    - oso: 0.7136\n",
            "    - perrito: 0.7128\n",
            "    - rottweiler: 0.7068\n",
            "  Similares (FastText):\n",
            "    - perros: 0.7588\n",
            "    - terrier: 0.6690\n",
            "    - perrito: 0.6587\n",
            "    - cachorro: 0.6577\n",
            "    - gato: 0.6526\n",
            "\n",
            "Palabra: 'volar'\n",
            "  Similares (Word2Vec):\n",
            "    - despegar: 0.7221\n",
            "    - aterrizar: 0.7009\n",
            "    - saltar: 0.6959\n",
            "    - volando: 0.6743\n",
            "    - vuela: 0.6551\n",
            "  Similares (FastText):\n",
            "    - volarse: 0.7906\n",
            "    - volar,: 0.7677\n",
            "    - volarlo: 0.7537\n",
            "    - volarlos: 0.7331\n",
            "    - volarle: 0.7325\n",
            "\n",
            "Palabra: 'artificial'\n",
            "  Similares (Word2Vec):\n",
            "    - Inseminación: 0.5876\n",
            "    - articial: 0.5663\n",
            "    - artifical: 0.5657\n",
            "    - Lago_Apanás: 0.5383\n",
            "    - multijuegos: 0.5181\n",
            "  Similares (FastText):\n",
            "    - bioartificial: 0.7892\n",
            "    - artificial»: 0.7790\n",
            "    - artificial—: 0.7639\n",
            "    - artificialmente: 0.7551\n",
            "    - artificials: 0.7375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Observaciones de la Comparación en Vocabulario ---\n",
        "\n",
        "¿Son las listas idénticas? Generalmente no.\n",
        "\n",
        "¿Muy parecidas? A menudo sí, con solapamiento significativo de las palabras principales.\n",
        "\n",
        "¿Diferentes? Pueden diferir en las palabras con menor puntuación o en matices sutiles.\n",
        "\n",
        "¿Cuál les parece 'mejor' o más coherente? Esto es subjetivo y depende del caso de uso.\n",
        "- **Word2Vec** tiende a ser muy bueno capturando relaciones semánticas precisas para palabras bien representadas en su corpus.\n",
        "- **FastText**, al considerar subpalabras, a veces puede capturar mejor las relaciones morfológicas y puede tener un 'sentido' más robusto incluso para palabras menos frecuentes, ya que la información de subpalabras contribuye a su vector.\n",
        "En general, para palabras comunes y bien representadas, ambos modelos suelen dar resultados muy coherentes y de alta calidad."
      ],
      "metadata": {
        "id": "ZcZD4QBYBX88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Test OOV Exhaustivo\n",
        "Ahora, realizaremos el test OOV exhaustivo con una lista propia de 10 palabras OOV."
      ],
      "metadata": {
        "id": "1NKPYtKbCM62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fasttext_vectors and word2vec_vectors: # Aseguramos que ambos modelos estén cargados\n",
        "    print(\"\\n--- Test OOV Exhaustivo ---\")\n",
        "\n",
        "    # Lista propia de 10 palabras OOV (ejemplos en español, asumiendo un modelo en español idealmente)\n",
        "    # Si los modelos cargados son en inglés, los resultados para estas palabras en español\n",
        "    # serán de 'KeyError' para Word2Vec y vectores menos significativos para FastText.\n",
        "    # Para una demostración real en español, necesitarías modelos pre-entrenados en español.\n",
        "    palabras_oov_personalizadas = [\n",
        "        \"hobmre\",          # Error tipográfico\n",
        "        \"qeu\",             # Error tipográfico común\n",
        "        \"dicimbre\",        # Error tipográfico\n",
        "        \"perrito\",         # Diminutivo\n",
        "        \"casitass\",        # Diminutivo/Aumentativo con error\n",
        "        \"libraco\",         # Aumentativo\n",
        "        \"habíamoscomido\",  # Forma verbal conjugada (sin espacio)\n",
        "        \"cantasteis\",      # Forma verbal menos frecuente\n",
        "        \"tecnoestrés\",     # Palabra inventada (tecnología + estrés)\n",
        "        \"computofilia\",    # Palabra inventada (computadora + filia)\n",
        "    ]\n",
        "\n",
        "    for palabra in palabras_oov_personalizadas:\n",
        "        print(f\"\\n--- Palabra OOV: '{palabra}' ---\")\n",
        "\n",
        "        # Verificar con Word2Vec\n",
        "        print(\"  Verificando con Word2Vec:\")\n",
        "        try:\n",
        "            vector_w2v = word2vec_vectors[palabra]\n",
        "            print(f\"    ¡Word2Vec encontró la palabra! (Esto es inesperado para una OOV real): {vector_w2v.shape}\")\n",
        "        except KeyError:\n",
        "            print(\"    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\")\n",
        "        except Exception as e:\n",
        "            print(f\"    Error inesperado con Word2Vec: {e}\")\n",
        "\n",
        "        # Obtener similares con FastText\n",
        "        print(\"  Obteniendo similares con FastText:\")\n",
        "        try:\n",
        "            vector_ft = fasttext_vectors[palabra]\n",
        "            print(f\"    FastText: Vector obtenido. (Dim: {vector_ft.shape})\")\n",
        "            similares_ft = fasttext_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"    Similares (FastText): {similares_ft}\")\n",
        "\n",
        "            # Análisis de sentido para FastText\n",
        "            print(\"    Análisis de sentido de similares (FastText):\")\n",
        "            for sim_palabra, score in similares_ft:\n",
        "                # Aquí la observación es manual: ¿La palabra similar tiene sentido con los componentes de la OOV?\n",
        "                # Ej: para \"computadorra\", ¿da \"computadora\", \"ordenador\"?\n",
        "                # Para \"perrito\", ¿da \"perro\", \"cachorro\"?\n",
        "                # Para \"tecnoestrés\", ¿da \"tecnología\", \"estrés\"?\n",
        "                print(f\"      - '{sim_palabra}': {score:.4f}\")\n",
        "            print(\"    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error inesperado con FastText: {e}\")\n",
        "            print(\"    (Este error es raro si el modelo FastText está cargado correctamente, ya que está diseñado para manejar OOV).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nAmbos modelos (Word2Vec y FastText) no están cargados para el test OOV exhaustivo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0uIy4DsC943",
        "outputId": "dd4d4746-9cd4-471e-e3a0-360ac6c34ab6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test OOV Exhaustivo ---\n",
            "\n",
            "--- Palabra OOV: 'hobmre' ---\n",
            "  Verificando con Word2Vec:\n",
            "    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('nobmre', 0.6312950253486633), ('ademre', 0.5202481746673584), ('demre', 0.5149299502372742)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'nobmre': 0.6313\n",
            "      - 'ademre': 0.5202\n",
            "      - 'demre': 0.5149\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'qeu' ---\n",
            "  Verificando con Word2Vec:\n",
            "    ¡Word2Vec encontró la palabra! (Esto es inesperado para una OOV real): (300,)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('qeuab', 0.6885186433792114), ('qeuad', 0.6670765280723572), ('wikisilki/pi', 0.6110560297966003)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'qeuab': 0.6885\n",
            "      - 'qeuad': 0.6671\n",
            "      - 'wikisilki/pi': 0.6111\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'dicimbre' ---\n",
            "  Verificando con Word2Vec:\n",
            "    ¡Word2Vec encontró la palabra! (Esto es inesperado para una OOV real): (300,)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('cimbres', 0.6492888331413269), ('cimbro', 0.6209564805030823), ('cimbros', 0.5852233171463013)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'cimbres': 0.6493\n",
            "      - 'cimbro': 0.6210\n",
            "      - 'cimbros': 0.5852\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'perrito' ---\n",
            "  Verificando con Word2Vec:\n",
            "    ¡Word2Vec encontró la palabra! (Esto es inesperado para una OOV real): (300,)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('perritos', 0.82405024766922), ('perrita', 0.7756099700927734), ('perritas', 0.7066512107849121)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'perritos': 0.8241\n",
            "      - 'perrita': 0.7756\n",
            "      - 'perritas': 0.7067\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'casitass' ---\n",
            "  Verificando con Word2Vec:\n",
            "    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('casitas', 0.7695338726043701), ('casita', 0.7024411559104919), ('casitagua', 0.6213944554328918)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'casitas': 0.7695\n",
            "      - 'casita': 0.7024\n",
            "      - 'casitagua': 0.6214\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'libraco' ---\n",
            "  Verificando con Word2Vec:\n",
            "    ¡Word2Vec encontró la palabra! (Esto es inesperado para una OOV real): (300,)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('libracos', 0.851459801197052), ('libración', 0.672081470489502), ('libraciones', 0.6215488314628601)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'libracos': 0.8515\n",
            "      - 'libración': 0.6721\n",
            "      - 'libraciones': 0.6215\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'habíamoscomido' ---\n",
            "  Verificando con Word2Vec:\n",
            "    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('habíamos', 0.694095253944397), ('comido', 0.6214999556541443), ('omido', 0.5806320309638977)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'habíamos': 0.6941\n",
            "      - 'comido': 0.6215\n",
            "      - 'omido': 0.5806\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'cantasteis' ---\n",
            "  Verificando con Word2Vec:\n",
            "    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('cantaste', 0.8618875741958618), ('fuisteis', 0.7056925296783447), ('gusteis', 0.7031436562538147)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'cantaste': 0.8619\n",
            "      - 'fuisteis': 0.7057\n",
            "      - 'gusteis': 0.7031\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'tecnoestrés' ---\n",
            "  Verificando con Word2Vec:\n",
            "    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('tecnoestructura', 0.8233116269111633), ('xenoestrógeno', 0.7390566468238831), ('xenoestrógenos', 0.7244404554367065)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'tecnoestructura': 0.8233\n",
            "      - 'xenoestrógeno': 0.7391\n",
            "      - 'xenoestrógenos': 0.7244\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n",
            "\n",
            "--- Palabra OOV: 'computofilia' ---\n",
            "  Verificando con Word2Vec:\n",
            "    **Word2Vec: KeyError.** Palabra no encontrada en su vocabulario. (¡Comportamiento esperado para OOV!)\n",
            "  Obteniendo similares con FastText:\n",
            "    FastText: Vector obtenido. (Dim: (300,))\n",
            "    Similares (FastText): [('sitofilia', 0.7233273386955261), ('textofilia', 0.7167273759841919), ('gerontofilia', 0.7099736928939819)]\n",
            "    Análisis de sentido de similares (FastText):\n",
            "      - 'sitofilia': 0.7233\n",
            "      - 'textofilia': 0.7167\n",
            "      - 'gerontofilia': 0.7100\n",
            "    ¿Los similares de FastText tienen sentido basado en las partes de la palabra OOV? (Observación manual)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Discusión: Impacto de la Capacidad OOV de FastText\n",
        "La capacidad de FastText para generar vectores para palabras fuera de su vocabulario (OOV) es una ventaja significativa sobre modelos como Word2Vec que operan a nivel de palabra completa y simplemente fallan (KeyError) cuando encuentran un término desconocido.\n",
        "\n",
        "¿En qué tipo de aplicaciones reales la capacidad OOV de FastText marcaría una diferencia significativa?\n",
        "\n",
        "Chatbots y Asistentes Virtuales de Atención al Cliente:\n",
        "\n",
        "¿Por qué? Los usuarios a menudo cometen errores tipográficos (\"Nesecito ayua\"), usan neologismos (\"whatsappear\", \"googlear\"), lenguaje coloquial o abreviaciones que no se vieron durante el entrenamiento. Word2Vec simplemente no entendería estas palabras, lo que llevaría a respuestas irrelevantes o a la incapacidad de procesar la consulta. FastText, al analizar subpalabras, puede inferir el significado de \"nesecito\" a partir de \"necesito\" o de \"wasapear\" a partir de \"whatsapp\", mejorando drásticamente la comprensión de la intención del usuario y la calidad del servicio.\n",
        "Sistemas de Recomendación de Noticias o Contenido:\n",
        "\n",
        "¿Por qué? Las noticias están en constante evolución, introduciendo nuevos nombres de personas, lugares, eventos (\"el presidente X\", \"la ciudad de Y\", \"la nueva variante Z\"). Un modelo Word2Vec se quedaría ciego ante estas novedades. FastText podría, por ejemplo, entender que \"covid-19\" (si no estaba en su vocabulario original) está relacionado con \"virus\" o \"pandemia\" debido a los n-gramas de caracteres compartidos, permitiendo que el sistema siga recomendando contenido relevante sobre temas emergentes.\n",
        "Correctores Ortográficos y Sugerencia de Texto:\n",
        "\n",
        "¿Por qué? Esta es una aplicación directa. Un corrector ortográfico basado en Word2Vec no sabría qué hacer con una palabra mal escrita. FastText, sin embargo, puede generar un vector para la palabra incorrecta y luego encontrar palabras similares en su vocabulario que tengan muchos n-gramas en común, lo que le permite sugerir la corrección correcta (ej., \"computadorra\" -> \"computadora\"). Esto mejora la precisión y la utilidad de estas herramientas.\n",
        "Análisis de Sentimiento en Redes Sociales o Plataformas de Opinión:\n",
        "\n",
        "¿Por qué? El lenguaje en redes sociales es muy dinámico, informal y propenso a errores tipográficos, abreviaturas y neologismos. Un modelo Word2Vec lucharía para capturar el sentimiento de frases que contienen muchas palabras OOV. FastText puede inferir el significado y, por ende, el sentimiento de esas palabras desconocidas, llevando a un análisis de sentimiento más robusto y preciso en contextos informales."
      ],
      "metadata": {
        "id": "th-ruVeTD7n4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Brainstorming: Evaluación y Detección de Sesgos\n",
        "\n",
        "Hemos visto diferentes formas de crear embeddings (Word2Vec, GloVe, FastText) y cómo usarlos. Pero, ¿cómo sabemos si un conjunto de embeddings es \"bueno\"? ¿Y cómo abordamos el problema de los sesgos que mencionamos en clases anteriores?\n",
        "\n",
        "**Pregunta:** **¿Cómo podemos evaluar la calidad de los word embeddings y detectar posibles sesgos?**\n",
        "\n",
        "*   **Evaluación Intrínseca:**\n",
        "    *   Medir qué tan bien funcionan los embeddings en tareas específicas relacionadas con las palabras mismas, **sin** una aplicación final.\n",
        "    *   Ejemplos:\n",
        "        *   **Similitud de Palabras:** Comparar la similitud coseno entre pares de palabras según los embeddings, con juicios de similitud dados por humanos (datasets estándar como WordSim-353, SimLex-999, adaptados o creados para español).\n",
        "        *   **Tareas de Analogía:** Ver qué tan bien resuelven analogías (`rey - hombre + mujer = ?`). Hay datasets estándar de analogías (Google Analogies, BATS).\n",
        "*   **Evaluación Extrínseca:**\n",
        "    *   Usar los embeddings como **características de entrada (features)** para un modelo de PLN más complejo que resuelve una tarea final (downstream task).\n",
        "    *   Ejemplos: Clasificación de sentimientos, reconocimiento de entidades nombradas, clasificación de temas.\n",
        "    *   Medir si usar estos embeddings mejora el rendimiento (precisión, F1-score, etc.) del sistema final comparado con no usarlos o usar otros embeddings. **Esta suele ser la evaluación más relevante en la práctica.**\n",
        "*   **Detección de Sesgos:**\n",
        "    *   **Analogías Específicas:** Crear analogías diseñadas para revelar sesgos sociales (género-profesión, raza-sentimiento, etc.). `programador - hombre + mujer = ?`.\n",
        "    *   **Pruebas de Asociación Implícita (como WEAT - Word Embedding Association Test):** Miden matemáticamente qué tan asociados están ciertos grupos de palabras (ej: nombres masculinos vs femeninos) con ciertos atributos (ej: carreras científicas vs artísticas, adjetivos positivos vs negativos) dentro del espacio de embeddings.\n",
        "    *   **Visualización:** A veces, proyectar grupos específicos (hombres/mujeres, profesiones) a 2D puede revelar agrupaciones o direcciones sesgadas.\n",
        "    *   **Auditoría de Vecinos Cercanos:** Ver los `most_similar` para palabras sensibles.\n",
        "\n",
        "**Una vez detectado el sesgo, ¿qué hacemos?** ¿Existen técnicas para mitigarlo (\"debiasing\")? ¿Es mejor buscar/crear corpus menos sesgados? ¿O simplemente ser transparentes sobre los sesgos del modelo?"
      ],
      "metadata": {
        "id": "x0BNPC_9minX"
      }
    }
  ]
}